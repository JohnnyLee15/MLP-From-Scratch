# MLP Neural Network in C++ ğŸ¤–

This is a **from-scratch, modular implementation of a Multi-Layer Perceptron (MLP)** written in modern C++. It supports classification and regression tasks, fast matrix operations, and intuitive data handling â€” all without external ML libraries.

## Features âœ¨

- ğŸ”§ **Fully customizable architecture**: any number of layers, neurons, and activation functions
- ğŸ§  **Supports classification and regression** with built-in task-specific utilities
- ğŸƒ **Efficient forward and backward propagation** with OpenMP parallelization
- ğŸ“† **Mini-batch SGD training** with learning rate decay
- ğŸ“‚ **Binary model saving/loading** (`.nn` format) with overwrite and rename support
- ğŸ§ª **Clear console UI** for data loading, model saving, error handling, and training progress
- ğŸ“Š **Evaluation utilities** for MAPE, accuracy, and progress bars
- ğŸ“ **CSV data loading** with automatic feature/target extraction and automatic one-hot encoding for categorical features
- ğŸŒˆ **Scaler support**: greyscale normalization and min-max scaling
- ğŸ› ï¸ No external ML libraries â€“ 100% custom C++

## Requirements âš™ï¸

- C++17 or later
- OpenMP (for multithreading)
- `make` (for building the project)
- Standard libraries only â€“ no third-party dependencies

## Supported Platforms ğŸ–¥ï¸

- **macOS** (via Homebrew + LLVM + libomp)
- **Linux** (Ubuntu/Debian)
- **Windows via WSL2** (Ubuntu or other Linux distros)

## Installing OpenMP ğŸ§©

If you're compiling with g++, OpenMP is typically included. To install it manually:

- **macOS**:

  - Make sure you have [Homebrew](https://brew.sh/) installed.
  - Then run:
  ```bash
  brew install libomp
  ```

- **Linux / WSL2**:

  ```bash
  sudo apt install libomp-dev
  ```

- âŒ Native Windows builds are not supported â€” use [WSL2](https://learn.microsoft.com/en-us/windows/wsl/).

## Usage ğŸš€

1. Clone the repository:

  ```bash
  git clone https://github.com/JohnnyLee15/MLPNeuralNetProject.git
  cd MLPNeuralNetProject
  ```

2. Compile the code:

  ```bash
  make
  ```

  The Makefile automatically includes the necessary OpenMP flags for parallelization support.

3. Run the program:

  Once compiled, you can run the MLP neural network:

  ```bash
  ./mlp
  ```

4. Clean the build:

  To remove the object files and compiled binary:

  ```bash
  make clean
  ```

## Data Format ğŸ“‚

- Input files must be in **CSV format**.
- The file must contain a **target column** (label or value).
- You can specify the **column name or index** for the target in the API:
  ```cpp
  data.readTrain("path.csv", "label");
  ```

## Model Saving / Loading ğŸ“‚

- Models are saved in compact **binary format** (`.nn`) using:

  ```cpp
  nn.saveToBin("model.nn");
  ```

- Existing files will prompt for:

  - `[q]` Cancel
  - `[o]` Overwrite
  - `[r]` Rename

- Models can be loaded using:

  ```cpp
  NeuralNet nn = NeuralNet::loadFromBin("model.nn");
  ```

## Customization ğŸ› ï¸

You can modify:

- Layer structure (in `Main.cpp`)
- Activation functions (`ReLU`, `Linear`, `Softmax`)
- Loss functions (`MSE`, `SoftmaxCrossEntropy`)
- Task type (`RegressionTask` or `ClassificationTask`)
- Training parameters:
  - `learningRate`
  - `decayRate`
  - `epochs`
  - `batchSize`

## Example Usage: MNIST Classification ğŸ¯

### Setup

```cpp
// Data Processing
Data data;
data.setTask(new ClassificationTask());
data.readTrain("DataFiles/mnist_train.csv", "label");
data.readTest("DataFiles/mnist_test.csv", "label");
data.setScalars(new Greyscale());                // setScalars(featureScalar, targetScalar); only featureScalar used for classification           
data.fitScalars();
size_t numFeatures = data.getTrainFeatures().getNumCols();

// Architecture
Loss *loss = new SoftmaxCrossEntropy();
vector<Layer*> layers = {
   new DenseLayer(64, numFeatures, new ReLU()), // Hidden layer 1: 64 neurons
   new DenseLayer(32, 64, new ReLU()),          // Hidden layer 2: 32 neurons
   new DenseLayer(10, 32, new Softmax())        // Output layer: 10 classes
};

// Instantiation
NeuralNet nn(layers, loss);

// Train
nn.train(
   data,
   0.01,  // learningRate
   0.01,  // decayRate
   3,     // epochs
   32     // batchSize
);

// Save
nn.saveToBin("modelTest.nn");

// Test
Matrix probs = nn.predict(data);
vector<double> predictions = TrainingUtils::getPredictions(probs);
double accuracy = TrainingUtils::getAccuracy(predictions, data.getTestTargets());
cout << "Test Accuracy: " << accuracy << endl;
```

### Output

```
============================================================
                  ğŸ§  MLP NEURAL NETWORK
               Lightweight C++ Neural Network
============================================================

ğŸ“¥ Loading training data from: "DataFiles/mnist_train.csv".
[âœ”] Loading Data.
[âœ”] Parsing Lines.
[âœ”] Extracting Features.
[âœ”] Extracting Targets.
------------------------------------------------------------

ğŸ“¥ Loading testing data from: "DataFiles/mnist_test.csv".
[âœ”] Loading Data.
[âœ”] Parsing Lines.
[âœ”] Extracting Features.
[âœ”] Extracting Targets.
------------------------------------------------------------

Epoch: 1/3
60000/60000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Accuracy: 82.30%| Avg Loss: 0.63 | Elapsed: 1.53s

Epoch: 2/3
60000/60000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Accuracy: 91.45%| Avg Loss: 0.30 | Elapsed: 1.51s

Epoch: 3/3
60000/60000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Accuracy: 92.84%| Avg Loss: 0.25 | Elapsed: 1.53s
------------------------------------------------------------
[âœ”] Model saved successfully as "modelTest.nn".
------------------------------------------------------------
Test Accuracy: 0.9323
```
---

## License âš–ï¸

MIT License â€“ see [LICENSE](https://opensource.org/licenses/MIT)

